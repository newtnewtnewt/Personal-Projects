---
title: "HW05"
author: "Noah Dunn"
date: "October 19, 2018"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
library(tidyverse)
library(knitr)
library(GGally)
library(lindia)
library(ggfortify)
```

###Problem 1 

```{r}
boarddata <- read.csv("particleboard.csv")
```


##1. 
```{r}
ggplot(boarddata, aes(x=Density, y=Stiffness)) +
  geom_point() + 
  geom_smooth() +               
  xlab("Density") +
  ylab("Stiffness")
```

There is a distinct curvature in this graph, noticeably curving upwards. 

##2. 

```{r}
boarddata.lm <- lm(Stiffness ~ Density, data= boarddata)
autoplot(boarddata.lm)
summary(boarddata.lm)
```

Normality appears to be fairly safe to utilize, although there is a single potentially drastic outlier. The residuals plot, much like the base scatter plot appears to increase as fitted values increase, which is not desired when looking for equal variance. 

##3. 

```{r}
gg_boxcox(boarddata.lm)
```

The range of transformation values occur over the interval 0-0.3. Given this information. A transformation of log(Stiffness) seems the most appropriate.

##4.

```{r}
boarddata <- boarddata %>%
  mutate(log.stiffness = log(Stiffness))
```  

##5.

```{r}
boarddatalog.lm <- lm(log.stiffness ~ Density, data= boarddata)
autoplot(boarddatalog.lm)
summary(boarddatalog.lm)
```

##6.
```{r}
boarddata <- boarddata %>%
  mutate(Lower.CI = exp(predict(boarddatalog.lm, interval="conf")[,2]),
         Upper.CI = exp(predict(boarddatalog.lm, interval="conf")[,3]),
         Lower.PI = exp(predict(boarddatalog.lm, interval="pred")[,2]),
         Upper.PI = exp(predict(boarddatalog.lm, interval="pred")[,3]),
  Fitted = exp(fitted(boarddatalog.lm)))

       

ggplot(boarddata) + 
  geom_ribbon(aes(x=Density, ymin=Lower.PI, ymax=Upper.PI), fill="gray80") + 
  geom_ribbon(aes(x=Density, ymin=Lower.CI, ymax=Upper.CI), fill="gray60") +
  geom_point(aes(x=Density, y=Stiffness) ) +
  geom_line(aes(x=Density, y=Fitted), color="royalblue") + 
  labs(x="Density", y="Stiffness") + 
  theme_bw()
```

##7.

We observe that as particleboard density increases, particleboard Stiffness also tends to increase at what appears to be an exponential rate. 

###Problem 2

```{r}
electiondata <- read.csv("election1992.csv")
```

##1. 
```{r}
electiondata.ohio <- electiondata %>%
  filter(State == "OH")
```

##2. 
```{r}
ggplot(electiondata.ohio, aes(x=Population, y=Percent)) +
  geom_point() + 
  geom_smooth() +               
  xlab("Population Density per square km of County in Ohio") +
  ylab("Percent voting for Bill Clinton")
```

There are a large number of counties with less than 300 people per square km.
Inside this scope there is a large variety of percentages, ranging from around 10% to over 60%. The few counties with population density > 300 people per square km are all above 30% voting for Bill Clinton.


##3.

```{r}
electiondata.ohio <- electiondata.ohio %>%
  
  mutate(Q1 = quantile(Population)[2],
         Median = quantile(Population)[3],
         Q3 = quantile(Population)[4],
         Quartile = case_when(Population < Q1 ~ "FirstQ",
                              Population >= Q1 & Population < Median ~ "SecondQ",
                              Population >= Median & Population <= Q3 ~ "ThirdQ",
                              Population > Q3 ~ "FourthQ"),
         Quartile = factor(Quartile, levels = c("FirstQ", "SecondQ", "ThirdQ", "FourthQ")))
 
```

##4.

```{r}
ggplot(electiondata.ohio,aes(x=Quartile,y =Percent)) + 
  geom_boxplot()+
  xlab("Population Rank by Quartile")+
  ylab("Percent Voted for Bill Clinton")+
  stat_summary(fun.y=mean, colour="blue", geom="point", 
                           shape=18, size=3,show_guide = FALSE)
```

##5.
```{r}
electiondata.ohio.lm <- lm(Percent ~ Population, data= electiondata.ohio)
autoplot(electiondata.ohio.lm)
summary(electiondata.ohio.lm)
```
Normality and equal variance of the residual error both appear to look fine 
for conducting analysis. However, this particular model has a p-value of 0.221, making it insignificant 
to do any form of prediction.

##6.
```{r}
electiondata.ohio.lm <- lm(Percent ~ Quartile, data= electiondata.ohio)
autoplot(electiondata.ohio.lm)
summary(electiondata.ohio.lm)
```

Normality and equal variance of the residual error of this set appear  sufficient. 
This model is also significant at the 95% confidence level.

##7.

The first model is very simple, as it does not have any categorization based on ranges and deals exclusively with measuring voting percentage based on an each counties' population density. Model 2 is more complex, as we deal with 4 distinct categories based on quartile ranges of population density.

The first model is not significant at any reasonable confidence level, and the second model is significant at a 95% confidence level. Within the model's significance, The Second Quartile experiences a negatively significant difference of 4.279 percent less people voting for Bill Clinton compared to the first quartile at a 90% confidence level.

The first model accounts for 0.6% of the variation in percent voting for Bill Clinton. The second accounts for 7.6% of the variation in percent voting for Bill Clinton.

I would argue the second model is better than the first, although neither are overtly compelling. The first suffer a low variation accountability, and it lacks any meaningful significance. The second model is significant, and although it only accounts for the significance of a single predictor at the 90% confidence level, this is better than no significance at all. It also possesses a higher variation accounted for.












